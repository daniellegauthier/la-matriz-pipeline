# -*- coding: utf-8 -*-
"""la matriz consulting pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CS_AhCEDehJYiIrl916y70h2VS3AjfR3
"""

import pandas as pd
import numpy as np
import requests
from io import StringIO
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.cluster import KMeans
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter, defaultdict
import nltk
import random
import re
import math


class NLTKPreprocessor(BaseEstimator, TransformerMixin):
    def __init__(self):
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            nltk.download('wordnet', quiet=True)
            self.lemmatizer = WordNetLemmatizer()
            self.stop_words = set(stopwords.words('english'))
            self.use_nltk = True
        except Exception as e:
            print(f"NLTK initialization failed: {e}. Using fallback tokenization.")
            self.use_nltk = False

    def preprocess(self, text):
        if self.use_nltk:
            try:
                tokens = word_tokenize(text.lower())
                filtered_tokens = [t for t in tokens if t.isalpha() and t not in self.stop_words]
                return [self.lemmatizer.lemmatize(t) for t in filtered_tokens]
            except:
                pass

        text = re.sub(r'[^\w\s]', '', text.lower())
        words = text.split()
        return [w for w in words if w not in self.stop_words and len(w) > 1]

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return [self.preprocess(text) for text in X]


class SemanticMapper(BaseEstimator, TransformerMixin):
    def __init__(self, url):
        self.url = url
        self.semantic_df = None
        self.word_to_rgb_map = {}
        self.rgb_to_words_map = defaultdict(list)
        self.all_semantic_words = set()

    def fetch_data(self):
        response = requests.get(self.url)
        if response.status_code == 200:
            self.semantic_df = pd.read_csv(StringIO(response.text))
            for col in ['R', 'G', 'B']:
                self.semantic_df[col] = pd.to_numeric(self.semantic_df[col], errors='coerce')
            return True
        return False

    def build_maps(self):
        for _, row in self.semantic_df.iterrows():
            rgb = (int(row['R']), int(row['G']), int(row['B']))
            rgb_key = f"{rgb[0]}_{rgb[1]}_{rgb[2]}"
            for col in ['Original Words', 'New Words']:
                if pd.notna(row[col]):
                    words = [w.strip().lower() for w in str(row[col]).split(',')]
                    for word in words:
                        self.word_to_rgb_map[word] = rgb
                        self.rgb_to_words_map[rgb_key].append(word)
                        self.all_semantic_words.add(word)

    def fit(self, X, y=None):
        if self.fetch_data():
            self.build_maps()
        return self

    def transform(self, X):
        return X


class PhraseAnalyzer(BaseEstimator, TransformerMixin):
    def __init__(self, word_to_rgb_map, all_semantic_words):
        self.word_to_rgb_map = word_to_rgb_map
        self.all_semantic_words = all_semantic_words

    def analyze(self, tokens):
        token_counts = Counter(tokens)
        seed_words = [t for t in tokens if t in self.word_to_rgb_map]
        if len(seed_words) < 3:
            seed_words += self._find_similar(tokens)
        return token_counts, seed_words[:3]

    def _find_similar(self, tokens, limit=3):
        similar_words = []
        for token in tokens:
            scores = []
            for word in self.all_semantic_words:
                if len(word) < 3:
                    continue
                overlap = len(set(token) & set(word)) / len(set(token) | set(word))
                prefix = sum(1 for i in range(min(len(token), len(word))) if token[i] == word[i]) / max(len(token), len(word))
                score = 0.6 * overlap + 0.4 * prefix
                if score > 0.3:
                    scores.append((word, score))
            scores.sort(key=lambda x: x[1], reverse=True)
            similar_words += [w for w, _ in scores[:limit]]
        return similar_words

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return [self.analyze(tokens) for tokens in X]


def generate_color_sequence(seed_words, word_to_rgb_map, semantic_df, length=6, coherence=0.7):
    rgb_sequence = []
    seed_rgbs = [word_to_rgb_map[word] for word in seed_words if word in word_to_rgb_map]
    current_rgb = random.choice(seed_rgbs) if seed_rgbs else tuple(semantic_df.sample(1)[['R', 'G', 'B']].values[0].astype(int))
    rgb_sequence.append(current_rgb)

    for _ in range(1, length):
        if random.random() < coherence:
            variation = random.randint(10, 50)
            new_rgb = tuple(np.clip([current_rgb[i] + random.randint(-variation, variation) for i in range(3)], 0, 255))
            current_rgb = tuple(semantic_df.iloc[((semantic_df[['R', 'G', 'B']] - new_rgb)**2).sum(1).idxmin()][['R', 'G', 'B']].astype(int))
        else:
            current_rgb = tuple(semantic_df.sample(1)[['R', 'G', 'B']].values[0].astype(int))
        rgb_sequence.append(current_rgb)
    return rgb_sequence


def calculate_momentum(rgb_sequence):
    momentum = []
    for i in range(1, len(rgb_sequence)):
        prev_rgb, curr_rgb = rgb_sequence[i-1], rgb_sequence[i]
        direction = [curr_rgb[j] - prev_rgb[j] for j in range(3)]
        magnitude = np.linalg.norm(direction)
        norm_direction = [d / magnitude if magnitude > 0 else 0 for d in direction]
        momentum.append({
            'distance': magnitude,
            'direction': direction,
            'normalized_direction': norm_direction,
            'magnitude': magnitude
        })
    return momentum


def visualize(rgb_sequence, momentum, semantic_df, rgb_to_words_map, title, input_phrase):
    fig = plt.figure(figsize=(15, 10))
    plt.suptitle(f"{title}\nInput: \"{input_phrase}\"", fontsize=16, y=0.98)

    ax1 = fig.add_subplot(211, projection='3d')
    r, g, b = zip(*rgb_sequence)
    ax1.plot(r, g, b, 'o-', linewidth=2, markersize=10)
    for i, (r_, g_, b_) in enumerate(rgb_sequence):
        ax1.scatter(r_, g_, b_, color=[r_/255, g_/255, b_/255], s=100)
        ax1.text(r_, g_, b_, f"{i+1}", fontsize=12)
    ax1.set_xlabel('Red (R)')
    ax1.set_ylabel('Green (G)')
    ax1.set_zlabel('Blue (B)')
    ax1.set_xlim(0, 255)
    ax1.set_ylim(0, 255)
    ax1.set_zlim(0, 255)

    ax2 = fig.add_subplot(212)
    for i, (r_, g_, b_) in enumerate(rgb_sequence):
        ax2.add_patch(plt.Rectangle((i, 0), 1, 1, color=[r_/255, g_/255, b_/255]))
        rgb_key = f"{r_}_{g_}_{b_}"
        words = rgb_to_words_map.get(rgb_key, [])[:2]
        if words:
            ax2.text(i + 0.5, 1.1, ", ".join(words), ha='center', va='bottom', fontsize=10, rotation=45)
        if i < len(momentum):
            m = momentum[i]['magnitude'] / 255
            ax2.add_patch(plt.Rectangle((i + 0.9, 0), 0.1, m, color='red', alpha=0.7))
    ax2.set_xlim(0, len(rgb_sequence))
    ax2.set_ylim(0, 2)
    ax2.set_title('Color Sequence with Semantic Words and Momentum')
    ax2.set_xlabel('Sequence Position')
    ax2.set_yticks([])
    plt.tight_layout()
    plt.subplots_adjust(top=0.9)
    plt.show()


# Execution
if __name__ == "__main__":
    input_phrases = ["i feel inspired but not creative"]
    pipeline = Pipeline([
        ('preprocessor', NLTKPreprocessor()),
        ('semantic_mapper', SemanticMapper(
            url="https://hebbkx1anhila5yf.public.blob.vercel-storage.com/semantic_rgb_mapping-7Fyy0MQQFX3s6KmXIryYY5kH3cG2qk.csv"
        )),
    ])

    pipeline.fit(input_phrases)
    preprocessed = pipeline.named_steps['preprocessor'].transform(input_phrases)
    mapper = pipeline.named_steps['semantic_mapper']

    analyzer = PhraseAnalyzer(mapper.word_to_rgb_map, mapper.all_semantic_words)
    analysis = analyzer.transform(preprocessed)

    for i, (counts, seeds) in enumerate(analysis):
        rgb_sequence = generate_color_sequence(seeds, mapper.word_to_rgb_map, mapper.semantic_df)
        momentum = calculate_momentum(rgb_sequence)
        visualize(rgb_sequence, momentum, mapper.semantic_df, mapper.rgb_to_words_map,
                  title="Color Sequence: Original Momentum", input_phrase=input_phrases[i])
